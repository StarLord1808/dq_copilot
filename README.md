# Data Quality Copilot CLI ğŸ¤–âœ¨

A command-line AI assistant that profiles tables, detects data quality issues, and auto-suggests dbt-style tests.

Think of it as a linting engine for your data â€” part spellchecker, part judgmental analyst, and slightly more useful than the average unsolicited LinkedIn advice.

## Why This Exists ğŸ˜«

Good analytics and ML depend on clean data. Unfortunately, writing data quality checks by hand ranks somewhere between watching paint dry and debugging CSV encodings as a hobby.

This tool automates the dreary bits by:

- **Profiling** your tables (no more manual eyeballing CSVs)
- **Detecting** anomalies using heuristics + an LLM assist
- **Suggesting** actionable fixes instead of vague doom messages
- **Generating** dbt-style tests automatically (yes, even the YAML)
- **Outputting** everything neatly for copy-paste or CI runs

## Architecture ğŸ—ï¸

This is more than a script â€” it's a tiny guild of specialized agents quietly judging your datasets.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CLI       â”‚  The Executive
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â–º TableLoaderAgent
       â”œâ”€â”€â–º ProfilerAgent
       â”œâ”€â”€â–º AnomalyDetectorAgent
       â”œâ”€â”€â–º TestGeneratorAgent
       â”œâ”€â”€â–º YamlGenerator
       â””â”€â”€â–º ReportRendererAgent
```

### Agent Roles

- **TableLoaderAgent**: Eats CSV, Parquet, etc. Handles encodings without emotional breakdowns.
- **ProfilerAgent**: Calculates stats, distributions, nulls, distinct counts â€” the accounting department.
- **AnomalyDetectorAgent**: Sherlock Holmes for data messiness:
  - Null swamps
  - Duplicate IDs
  - Boring constant columns
  - Negative numbers pretending they belong
- **TestGeneratorAgent**: Uses GPT to generate human-grade recommendations and dbt tests.
- **ReportRendererAgent**: Pretty terminal output because aesthetics matter.

## Setup ğŸ› ï¸

### Prerequisites

- Python 3.8+
- OpenAI API Key (recommended unless you love rule engines)
- Coffee â˜• (optional but statistically correlated with productivity)

### Installation

```bash
cd /home/jiraiya/codebase/ai-agent/dq-copilot
python3 -m venv venv
source venv/bin/activate
pip install -e .
export OPENAI_API_KEY="your-key-here"
```

## Usage ğŸš€

### Profile Mode â€” Stats Only

```bash
dq-copilot profile --table-path examples/orders.csv --table-name orders
```

**Produces:** `orders_profile.json`

### Full Run â€” Profiling + AI Insights + Tests

```bash
dq-copilot run --table-path examples/orders.csv --table-name orders
```

**Outputs:**
- `orders_profile.json`
- `tests/orders_tests.yml`

### Flags

```
--table-path PATH    File location
--table-name NAME    Logical table name
--output-dir DIR     Output destination
--api-key KEY        LLM key
```

## Example Demo ğŸ¿

A deliberately messy sample (`examples/orders.csv`) is available. Running the tool gives a structured output summarizing issues, priorities, suggested actions, and dbt tests â€” like a performance review, except useful.

**Example highlights:**
- Unique ID failures flagged as **CRITICAL**
- Null explosions flagged as **HIGH**
- Negative values politely interrogated

Reports include remediation steps and generated dbt tests (`not_null`, `unique`, ranges, etc.).

## Configuration âš™ï¸

### LLM Integration

Defaults to OpenAI GPT-4. Without a key, the tool falls back to rule-based analysis â€” functional, but a bit rotary-phone-in-smartphone-world.

### Thresholds

Customizable heuristics:

```python
AnomalyDetectorAgent(
    high_null_threshold=0.3,
    constant_threshold=1,
    id_uniqueness_threshold=1.0
)
```

## Dependencies ğŸ“¦

- `click` â€” CLI framework
- `pandas` â€” Data wrangler's Swiss army knife
- `pyarrow` â€” Parquet support
- `openai` â€” AI brainpower
- `pyyaml` â€” dbt output whisperer
- `rich` â€” Pretty output, because CLI doesn't have to be sad

## Development ğŸ’»

```bash
pip install -e ".[dev]"
pytest
black dq_copilot/
ruff check dq_copilot/
```

## License ğŸ“œ

MIT â€” experiment, extend, or creatively destroy. Just don't blame us for philosophical crises induced by bad data.

---

*Built with caffeine, curiosity, and an unreasonable love for well-behaved datasets.*